{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module import \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "import pymysql \n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "pymysql.install_as_MySQLdb()\n",
    "import MySQLdb\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pickle\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class youtube_analysis:\n",
    "    \n",
    "    \"\"\"\n",
    "    1. Youtube main search contents information crawling\n",
    "    2. In Youtube main, each video's information crawling, ex) running time, comment, etc..\n",
    "    3. Distinguish Each video's language \n",
    "    4. Make a Wordcloud\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Youtube analysis\")\n",
    "    \n",
    "    \n",
    "    ##  ----Youtube Crawling function line----\n",
    "    \n",
    "    def search_main(self,search_object,chrome_driver_root,filter = None):\n",
    "        \n",
    "        # input information\n",
    "        # search_object = object that you want to searching in youtube\n",
    "        # chrome_driver_root = your computer's root that exists chrome driver\n",
    "        \n",
    "        # time_filter = filter how latest you want to crawl youtube data,\n",
    "        ### default value none_filter\n",
    "        ### 'hour','year','week','month','day'\n",
    "        \n",
    "        chrome_driver = chrome_driver_root\n",
    "        driver = webdriver.Chrome(chrome_driver)\n",
    "\n",
    "        ### time filter, this is empirical, you may need to edit later ### \n",
    "        oneday = \"&sp=EgQIAhAB\"\n",
    "        oneweek = \"&sp=EgQIAxAB\"\n",
    "        onemonth = \"&sp=EgQIBBAB\"\n",
    "        oneyear = \"&sp=EgQIBRAB\"\n",
    "        onehour = \"&sp=EgQIARAB\"\n",
    "        none_filter = ''\n",
    "        \n",
    "        if filter == 'hour' :\n",
    "            time_filter = onehour\n",
    "        elif filter == 'year':\n",
    "            time_filter = oneyear\n",
    "        elif filter == 'week' :\n",
    "            time_filter = oneweek\n",
    "        elif filter == 'month' :\n",
    "            time_filter = onemonth\n",
    "        elif filter == 'day':\n",
    "            time_filter = oneday\n",
    "        else :\n",
    "            time_filter = none_filter\n",
    "        \n",
    "        # url that you want to search that object and start driver\n",
    "        search_input = search_object\n",
    "        url = 'https://www.youtube.com/results?search_query={}{}'.format(search_object, time_filter)\n",
    "        driver.get(url)\n",
    "\n",
    "        last_page_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "        # selenium auto-scroll function\n",
    "        # if youtube page is ended, than chrome driver page is down\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "            \n",
    "            time.sleep(2.0) # sleep for computer's crawling loading \n",
    "            \n",
    "            new_page_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            if new_page_height == last_page_height:\n",
    "                break\n",
    "            last_page_height = new_page_height\n",
    "\n",
    "\n",
    "        src = driver.page_source\n",
    "        soup = BeautifulSoup(src)\n",
    "        driver.close()\n",
    "\n",
    "        data = soup.select('#video-title')\n",
    "\n",
    "        print('the number : ',len(data),' title loaded')\n",
    "\n",
    "        date_list = []\n",
    "        title_list = []\n",
    "        uploader_list = []\n",
    "        views_list = []\n",
    "        running_time_list = []\n",
    "        link_list = []\n",
    "        \n",
    "        \n",
    "        # saving and preprocessing information \n",
    "        for i in range(len(data)):\n",
    "           \n",
    "            uploaded_time = datetime.datetime.now()\n",
    "            \n",
    "            if 'class=\"yt-simple-endpoint style-scope ytd-video-renderer\"' in str(data[i]):\n",
    "                pass\n",
    "            else :\n",
    "                continue\n",
    "            contents = str(data[i]).split('class=\"yt-simple-endpoint style-scope ytd-video-renderer\"')[0].strip('<a aria-label=')\n",
    "            \n",
    "            need_to_strip = contents[0]\n",
    "            contents = contents.strip(need_to_strip)\n",
    "            \n",
    "            link = str(data[i]).split('class=\"yt-simple-endpoint style-scope ytd-video-renderer\"')[1].split('id=\"video-title\"')[0].strip(' href=\"')\n",
    "            views = contents.split(' 조회수 ')[-1].split('회')[0]\n",
    "            \n",
    "            if contents.split(' 조회수 ')[-2].split(' 전')[-1] == '':\n",
    "                running_time = ''\n",
    "            else : \n",
    "                running_time = contents.split(' 조회수 ')[-2].split(' 전 ')[-1]\n",
    "\n",
    "            upload_time = contents.split(' 조회수 ')[-2].split(' 전 ')[-2].split(' ')[-1]\n",
    "\n",
    "            \n",
    "            # time preprocessing\n",
    "            if '초' in upload_time :\n",
    "                delta = int(upload_time.strip('초'))\n",
    "                uploaded_time -= datetime.timedelta(seconds=delta)\n",
    "                uploaded_time = uploaded_time.date()\n",
    "            elif '분' in upload_time :\n",
    "                delta = int(upload_time.strip('분'))\n",
    "                uploaded_time -= datetime.timedelta(minutes = delta)\n",
    "                uploaded_time = uploaded_time.date()\n",
    "            elif '시간' in upload_time :\n",
    "                delta = int(upload_time.strip('시간'))\n",
    "                uploaded_time -= datetime.timedelta(hours = delta)\n",
    "                uploaded_time = uploaded_time.date()\n",
    "            elif '일' in upload_time :\n",
    "                delta = int(upload_time.strip('일'))\n",
    "                uploaded_time -= datetime.timedelta(days = delta)\n",
    "                uploaded_time = uploaded_time.date()\n",
    "            elif '주' in upload_time:\n",
    "                delta = int(upload_time.strip('주'))\n",
    "                uploaded_time -= datetime.timedelta(weeks = delta)\n",
    "                uploaded_time = uploaded_time.date()\n",
    "            elif '개월' in upload_time:\n",
    "                delta = int(upload_time.strip('개월'))\n",
    "                uploaded_time -= relativedelta(months=delta)\n",
    "                uploaded_time = uploaded_time.date()\n",
    "            elif '년' in upload_time :\n",
    "                delta = int(upload_time.strip('년'))\n",
    "                uploaded_time -= relativedelta(years=3)\n",
    "                uploaded_time = uploaded_time.date()\n",
    "            else :\n",
    "                pass \n",
    "\n",
    "            if running_time == '' :\n",
    "                uploader = contents.split(' 조회수 ')[-2].split(' 전')[-2].split(' 게시자: ')[-1].strip(upload_time).strip()\n",
    "                contents = contents.split(' 조회수 ')[-2].split(' 전')[-2].split(' 게시자: ')[0]\n",
    "            else :\n",
    "                uploader = contents.split(' 조회수 ')[-2].split(' 전 ')[-2].split(' 게시자: ')[-1].strip(upload_time).strip()\n",
    "                contents = contents.split(' 조회수 ')[-2].split(' 전 ')[-2].split(' 게시자: ')[0]\n",
    "            \n",
    "            date_list.append(uploaded_time)\n",
    "            title_list.append(contents)\n",
    "            uploader_list.append(uploader)\n",
    "            views_list.append(views)\n",
    "            running_time_list.append(running_time)\n",
    "            link_list.append(link)\n",
    "            \n",
    "\n",
    "        df_search_youtube = pd.DataFrame({\"date\" : date_list,\"title\" : title_list,\"uploader\" : uploader_list,\"views\" : views_list,\"running_time\" : running_time_list,'url': link_list})\n",
    "        df_search_youtube.views = df_search_youtube.views.str.replace('없음', '0')\n",
    "        df_search_youtube.views = df_search_youtube.views.str.replace(',', '').astype('int64')\n",
    "        df_search_youtube.drop_duplicates([\"url\"],inplace = True)\n",
    "\n",
    "        # output information\n",
    "        # type : pd.dataframe\n",
    "        # column : uploaded_date, video_title, uploader, views, video_running_time, url\n",
    "        \n",
    "        return df_search_youtube\n",
    "    \n",
    "    def search_comment(self,url,chrome_driver_root):\n",
    "        \n",
    "        \n",
    "        chrome_driver = chrome_driver_root\n",
    "        driver = webdriver.Chrome(chrome_driver)\n",
    "        link = \"https://www.youtube.com\"+url\n",
    "        driver.get(link)\n",
    "\n",
    "        last_page_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "        # selenium auto-scroll function\n",
    "        # if youtube page is ended, than chrome driver page is down\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "            \n",
    "            time.sleep(2.0) # sleep for computer's crawling loading \n",
    "            \n",
    "            new_page_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            if new_page_height == last_page_height:\n",
    "                break\n",
    "            last_page_height = new_page_height\n",
    "\n",
    "        src = driver.page_source\n",
    "        soup = BeautifulSoup(src,'lxml')\n",
    "        driver.close()\n",
    "        \n",
    "        user_ID = soup.select('div#header-author > a > span')\n",
    "        comment_time = soup.select(\"#header-author > yt-formatted-string > a\")\n",
    "        comment_text = soup.select(\"#content-text\")\n",
    "        comment_like = soup.select(\"span#vote-count-left\")\n",
    "\n",
    "        userID_list = [] \n",
    "        comments_list = []\n",
    "        comment_time_list = []\n",
    "        like_list = []\n",
    "        url_list = []\n",
    "\n",
    "        for i in range(len(user_ID)): \n",
    "            userID_tmp = str(user_ID[i].text) \n",
    "            # print(str_tmp) \n",
    "            userID_tmp = userID_tmp.replace('\\n', '') \n",
    "            userID_tmp = userID_tmp.replace('\\t', '') \n",
    "            userID_tmp = userID_tmp.replace(' ','') \n",
    "            userID_list.append(userID_tmp) \n",
    "\n",
    "            comment_time_tmp = str(comment_time[i].text) \n",
    "            # print(str_tmp) \n",
    "            comment_time_tmp = comment_time_tmp.replace('\\n', '') \n",
    "            comment_time_tmp = comment_time_tmp.replace('\\t', '') \n",
    "            comment_time_tmp = comment_time_tmp.replace(' 전','')\n",
    "            comment_time_tmp = comment_time_tmp.replace('(수정됨)','') \n",
    "            commented_time = datetime.datetime.now()\n",
    "\n",
    "            if '초' in comment_time_tmp :\n",
    "                delta = int(comment_time_tmp.strip('초'))\n",
    "                commented_time -= datetime.timedelta(seconds=delta)\n",
    "                commented_time = commented_time.date()\n",
    "            elif '분' in comment_time_tmp :\n",
    "                delta = int(comment_time_tmp.strip('분'))\n",
    "                commented_time -= datetime.timedelta(minutes = delta)\n",
    "                commented_time = commented_time.date()\n",
    "            elif '시간' in comment_time_tmp :\n",
    "                delta = int(comment_time_tmp.strip('시간'))\n",
    "                commented_time -= datetime.timedelta(hours = delta)\n",
    "                commented_time = commented_time.date()\n",
    "            elif '일' in comment_time_tmp :\n",
    "                delta = int(comment_time_tmp.strip('일'))\n",
    "                commented_time -= datetime.timedelta(days = delta)\n",
    "                commented_time = commented_time.date()\n",
    "            elif '주' in comment_time_tmp:\n",
    "                delta = int(comment_time_tmp.strip('주'))\n",
    "                commented_time -= datetime.timedelta(weeks = delta)\n",
    "                commented_time = commented_time.date()\n",
    "            elif '개월' in comment_time_tmp:\n",
    "                delta = int(comment_time_tmp.strip('개월'))\n",
    "                commented_time -= relativedelta(months=delta)\n",
    "                commented_time = commented_time.date()\n",
    "            elif '년' in comment_time_tmp:\n",
    "                delta = int(comment_time_tmp.strip('년'))\n",
    "                commented_time -= relativedelta(years=3)\n",
    "\n",
    "            else :\n",
    "                pass\n",
    "            comment_time_list.append(commented_time) \n",
    "\n",
    "            comments_tmp = str(comment_text[i].text) \n",
    "            comments_tmp = comments_tmp.replace('\\n', ' ') \n",
    "            comments_tmp = comments_tmp.replace('\\t', ' ') \n",
    "            comments_tmp = comments_tmp.strip(' ') \n",
    "            comments_list.append(comments_tmp)\n",
    "\n",
    "            like_tmp = str(comment_like[i].text) \n",
    "            # print(str_tmp) \n",
    "            like_tmp = like_tmp.replace('\\n', '') \n",
    "            like_tmp = like_tmp.replace('\\t', '') \n",
    "            like_tmp = like_tmp.replace(' ','')\n",
    "\n",
    "            if '천' in like_tmp :\n",
    "                like_tmp = like_tmp.split('천')[0]\n",
    "                like_tmp = float(like_tmp) * 1000\n",
    "\n",
    "            like_tmp = int(like_tmp)\n",
    "            like_list.append(like_tmp)  \n",
    "\n",
    "            url_list.append(url)\n",
    "\n",
    "        df_comment_youtube = pd.DataFrame({\"date\" : comment_time_list,\"userID\" : userID_list,\"comment\" : comments_list,\"likes\" : like_list,'url': url_list})\n",
    "        df_comment_youtube[\"date\"] = df_comment_youtube[\"date\"].astype(str)\n",
    "        \n",
    "        print(len(userID_list),'comments loaded')\n",
    "        \n",
    "        return df_comment_youtube\n",
    "        \n",
    "    ##  ----Youtube Mysql function line----\n",
    "    \n",
    "    \n",
    "    def Mysql_df_save(self,df_save,id_,pw_,host,DB,table_name):\n",
    "        \n",
    "        engine = create_engine(\"mysql+mysqldb://{}:\".format(id_)+\"{}\".format(pw_)+\"@{}/{}\".format(host,DB), encoding='utf-8')\n",
    "        conn = engine.connect()\n",
    "        df_save.to_sql(name='{}'.format(table_name), con=engine, if_exists='append',index=False)\n",
    "        \n",
    "        print(\"=====Saving dataframe to Mysql is completed=====\")\n",
    "\n",
    "    def Mysql_df_load(self,id_,pw_,host,port,DB,table_name):\n",
    "        \n",
    "        db = pymysql.connect(host = '{}'.format(host), port = port,user ='{}'.format(id_),passwd = '{}'.format(pw_),db='{}'.format(DB),charset = 'utf8')\n",
    "        cursor = db.cursor()\n",
    "        sql = \"\"\" select * from {} ;\n",
    "              \"\"\".format(table_name)\n",
    "        cursor.execute(sql)\n",
    "        data = pd.DataFrame(cursor.fetchall())\n",
    "        \n",
    "        return data\n",
    "\n",
    "    \n",
    "    ##  ----Youtube language function line----\n",
    "    ## if you want to practice only korean analysis, than you don't need this functions\n",
    "    \n",
    "    def isHangul(self, text):\n",
    "        hanCount = len(re.findall(u'[\\u3130-\\u318F\\uAC00-\\uD7A3]+', text))\n",
    "        if hanCount > 0 :\n",
    "            return True\n",
    "        else :\n",
    "            return False\n",
    "        \n",
    "    def isEnglish(self,text):\n",
    "    \n",
    "    \n",
    "        text_list = text.split()\n",
    "        wrong_count = 0\n",
    "        correct_count = 0\n",
    "    \n",
    "        for idx in text_list :\n",
    "\n",
    "            if 'en' == langid.classify(idx)[0] :\n",
    "                correct_count += 1\n",
    "            else :\n",
    "                wrong_count+= 1\n",
    "        \n",
    "        # when there is English name in Search title(for translation) \n",
    "        if correct_count > wrong_count + 1 :\n",
    "            return True\n",
    "        else :\n",
    "            return False\n",
    "        \n",
    "    def classifying_language(self, dataframe):\n",
    "    \n",
    "        # output list\n",
    "        Kor_url_list = []\n",
    "        Eng_url_list = []\n",
    "        Etc_url_list = []\n",
    "\n",
    "        for index in dataframe.index:\n",
    "            if isHangul(dataframe.loc[index].title):\n",
    "                Kor_url_list.append(dataframe.loc[index].url)\n",
    "            elif isEnglish(dataframe.loc[index].title):\n",
    "                Eng_url_list.append(dataframe.loc[index].url)\n",
    "            else : \n",
    "                text_list = dataframe.loc[index].title.split()\n",
    "                \n",
    "                # Sometimes, distribution error happened\n",
    "                # this err is empirical detection\n",
    "                wrong_count = 0\n",
    "                correct_count = 0\n",
    "\n",
    "                for idx in text_list :\n",
    "                    if 'ko' == langid.classify(idx)[0] :\n",
    "                        correct_count += 1\n",
    "                    else :\n",
    "                        wrong_count+= 1\n",
    "\n",
    "                if correct_count > wrong_count :\n",
    "                    Kor_url_list.append(dataframe.loc[index].url)\n",
    "\n",
    "                else :     \n",
    "                    Etc_url_list.append(dataframe.loc[index].url)\n",
    "\n",
    "        return Kor_url_list, Eng_url_list, Etc_url_list\n",
    "    \n",
    "    ##  ----restrict timeline----\n",
    "    \n",
    "    def select_date(self,df_input,str_start,str_end):\n",
    "    \n",
    "        # type str date input change to type datetime\n",
    "        dt_start = datetime.datetime.strptime(str_start,\"%Y-%m-%d\").date()\n",
    "        dt_end = datetime.datetime.strptime(str_end,\"%Y-%m-%d\").date()\n",
    "        start_to_df_input = df_input[df_input[\"date\"] >= dt_start]\n",
    "        start_to_end_df_input = start_to_df_input[start_to_df_input[\"date\"] <= dt_end]\n",
    "\n",
    "        return start_to_end_df_input\n",
    "    \n",
    "    ##  ----distribute morpheme----\n",
    "    \n",
    "    def tokenizer_lang_class(self, df):\n",
    "    \n",
    "        print(\"-------------- tokenizing language start ----------------\")    \n",
    "        okt = Okt()\n",
    "        res_token_df = pd.DataFrame(columns = ['date','token','likes','type'])\n",
    "        df.reset_index(drop = True,inplace=True)\n",
    "        k = 0     \n",
    "\n",
    "        for idx in df.index :\n",
    "            # step 출력\n",
    "            if idx % 100 == 0 :\n",
    "                print(\"step = \",idx, df.loc[idx])\n",
    "\n",
    "            # tokenize\n",
    "            token_list = okt.pos(df.loc[idx].comment) \n",
    "\n",
    "            # 새로운 데이터 프레임에 적용\n",
    "            for token in token_list :    \n",
    "                if token[-1] in ['Noun','Verb','Adjective'] :\n",
    "                    res_token_df.loc[k] = [df.loc[idx].date,token[0],df.loc[idx].likes,token[-1]]\n",
    "                    k += 1\n",
    "\n",
    "        N_token = res_token_df[res_token_df[\"type\"] == 'Noun']\n",
    "        V_token = res_token_df[res_token_df[\"type\"] == 'Verb']\n",
    "        A_token = res_token_df[res_token_df[\"type\"] == 'Adjective']         \n",
    "\n",
    "        return N_token, V_token, A_token\n",
    "    \n",
    "    def morpheme_distribution(self,df):\n",
    "\n",
    "        print(\"-------------- morpheme_distribution start ----------------\")\n",
    "        kkma = Kkma()\n",
    "        NN_token_df = pd.DataFrame(columns = ['date','token','likes','type'])\n",
    "        VV_token_df = pd.DataFrame(columns = ['date','token','likes','type'])\n",
    "        VA_token_df = pd.DataFrame(columns = ['date','token','likes','type'])\n",
    "        etc_token_df = pd.DataFrame(columns = ['date','token','likes','type'])\n",
    "\n",
    "        df.reset_index(drop = True,inplace=True)\n",
    "        k = 0     \n",
    "\n",
    "        for idx in df.index :\n",
    "            # step 출력\n",
    "            if idx % 100 == 0 :\n",
    "                print(\"step = \",idx, df.loc[idx])\n",
    "\n",
    "            # tokenize\n",
    "            token_list = kkma.pos(df.loc[idx].comment) \n",
    "\n",
    "            temp_token = ''\n",
    "            for token in token_list :\n",
    "                if 'N' in token[-1]: \n",
    "                    temp_token += token[0]\n",
    "            for token in token_list :\n",
    "                if 'VV' == token[-1]:\n",
    "                    VV_token_df.loc[k] = [df.loc[idx].date,token[0],df.loc[idx].likes,token[-1]]\n",
    "                elif 'VA' == token[-1]: \n",
    "                    VA_token_df.loc[k] = [df.loc[idx].date,token[0],df.loc[idx].likes,token[-1]]\n",
    "                elif 'NN' in token[-1]: \n",
    "                    NN_token_df.loc[k] = [df.loc[idx].date,token[0],df.loc[idx].likes,token[-1]]\n",
    "                else :\n",
    "                    etc_token_df.loc[k] = [df.loc[idx].date,token[0],df.loc[idx].likes,token[-1]]\n",
    "            k += 1\n",
    "        NN_token_df.reset_index(drop = True,inplace=True)\n",
    "        VV_token_df.reset_index(drop = True,inplace=True)\n",
    "        VA_token_df.reset_index(drop = True,inplace=True)\n",
    "        etc_token_df.reset_index(drop = True,inplace=True)\n",
    "\n",
    "        return NN_token_df, VV_token_df, VA_token_df, etc_token_df\n",
    "    \n",
    "    def save_pickle(self, df,name):\n",
    "        df.to_pickle(\"{}\".format(name))\n",
    "        print(\"{} save is completed\".format(name))\n",
    "    \n",
    "    ##  ----making word cloud----\n",
    "    \n",
    "    def like_base_list(self,df,rank):\n",
    "\n",
    "        like_multiple_list = []\n",
    "\n",
    "        for idx in df.index:\n",
    "        \n",
    "            for count in range(df.loc[idx].likes):\n",
    "                like_multiple_list.append(df.loc[idx].token)\n",
    "        \n",
    "        return like_multiple_list\n",
    "    \n",
    "    def make_wordcloud(self,list_,count,type_,save_name):\n",
    "\n",
    "        counts = Counter(list_) \n",
    "\n",
    "        words = dict(counts.most_common(count))\n",
    "\n",
    "        font_path = 'c:\\\\windows\\\\fonts\\\\NanumGothic.ttf'\n",
    "        wc = WordCloud(font_path= font_path ,\n",
    "                       background_color=\"white\",\n",
    "                       colormap = \"Accent_r\",\n",
    "                       width = 1500,\n",
    "                       height = 1000\n",
    "                       ).generate_from_frequencies(words)\n",
    "\n",
    "        plt.imshow(wc)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        plt.imshow(wc)\n",
    "        plt.savefig('wordcloud_{}'.format(save_name),dpi = 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \\'__main__\\':\\n    start_time = time.time()\\n    driver_root = \\'C:/Users/user/Desktop/머신러닝과 데이터분석/chromedriver_win32/chromedriver.exe\\'\\n    \\n    \\n    analysis = youtube_analysis()\\n    \\n    # no1 - 필라이트 이미지 감정분석\\n    main_data = analysis.search_main(search_object =  \\'\\'\\'\"편의점 맥주\"\\'\\'\\'\\n                                     ,chrome_driver_root = driver_root\\n                                     ,filter = \\'year\\')\\n    \\n    comment_df = pd.DataFrame(columns = [\"comment_date\" ,\"userID\" ,\"likes\" ,\\'url\\'])\\n    cnt = 1\\n    try :\\n        for url in main_data[\\'url\\']:\\n            comment_df = pd.concat([comment_df,analysis.search_comment(url,driver_root)])\\n            \\n            if cnt % 50 == 0:\\n                print(\"count :\",cnt)\\n            cnt+=1\\n    except Exception :\\n        if cnt % 50 == 0:\\n                print(\"count :\",cnt)\\n        print(url,\"Exception occurs\")\\n        cnt += 1\\n    \\n    print(\"---- %s seconds ----\" % (time.time() - start_time))\\n    \\n    analysis.Mysql_df_save(main_data,\\'root\\',\\'sogangsp\\',\\'localhost\\',\\'sns_db\\',\\'youtube_search_convenience\\')\\n    analysis.Mysql_df_save(comment_df,\\'root\\',\\'sogangsp\\',\\'localhost\\',\\'sns_db\\',\\'youtube_comment_convenience\\')\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### crawling\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    driver_root = 'C:/Users/user/Desktop/머신러닝과 데이터분석/chromedriver_win32/chromedriver.exe'\n",
    "    \n",
    "    \n",
    "    analysis = youtube_analysis()\n",
    "    \n",
    "    # no1 - 필라이트 이미지 감정분석\n",
    "    main_data = analysis.search_main(search_object =  '''\"편의점 맥주\"'''\n",
    "                                     ,chrome_driver_root = driver_root\n",
    "                                     ,filter = 'year')\n",
    "    \n",
    "    comment_df = pd.DataFrame(columns = [\"comment_date\" ,\"userID\" ,\"likes\" ,'url'])\n",
    "    cnt = 1\n",
    "    try :\n",
    "        for url in main_data['url']:\n",
    "            comment_df = pd.concat([comment_df,analysis.search_comment(url,driver_root)])\n",
    "            \n",
    "            if cnt % 50 == 0:\n",
    "                print(\"count :\",cnt)\n",
    "            cnt+=1\n",
    "    except Exception :\n",
    "        if cnt % 50 == 0:\n",
    "                print(\"count :\",cnt)\n",
    "        print(url,\"Exception occurs\")\n",
    "        cnt += 1\n",
    "    \n",
    "    print(\"---- %s seconds ----\" % (time.time() - start_time))\n",
    "    \n",
    "    analysis.Mysql_df_save(main_data,'root','sogangsp','localhost','sns_db','youtube_search_convenience')\n",
    "    analysis.Mysql_df_save(comment_df,'root','sogangsp','localhost','sns_db','youtube_comment_convenience')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
